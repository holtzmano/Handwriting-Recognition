{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBV2kJ8muB4h"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W1IyuS7fGDG7"
   },
   "outputs": [],
   "source": [
    "PICTURE_SIZE = 204\n",
    "PATCH_SIZE = 180\n",
    "EPOCHS = 1\n",
    "FILE_PATH = r\"C:/Users/lenovo/Desktop/DL_data/5/patches/patches/\"\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.0005\n",
    "PARTITION_PATH = 'C:/Users/lenovo/Desktop/DL_data/5/partition_data.pkl'\n",
    "LABELS_PATH = 'C:/Users/lenovo/Desktop/DL_data/5/labels_data.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZzN7qv0YWiY"
   },
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=BATCH_SIZE, dim=(PATCH_SIZE, PATCH_SIZE), n_channels=1,\n",
    "                 n_classes=PICTURE_SIZE, shuffle=True, data_gen = True):\n",
    "        'Initialization' #\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        self.data_gen = data_gen\n",
    "\n",
    "        self.datagen = ImageDataGenerator(\n",
    "          rotation_range=20,\n",
    "          width_shift_range=0.2,\n",
    "          height_shift_range=0.2,\n",
    "          horizontal_flip=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples'\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            filepath = FILE_PATH\n",
    "            loaded_data = np.load(filepath + ID + '.npy')\n",
    "            loaded_data = np.expand_dims(loaded_data, axis=-1)\n",
    "            if self.data_gen:\n",
    "              loaded_data = self.datagen.random_transform(loaded_data)\n",
    "\n",
    "            X[i,] = loaded_data\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tzFMNAzIG6Gm"
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {'dim': (PATCH_SIZE, PATCH_SIZE),\n",
    "          'batch_size': BATCH_SIZE,\n",
    "          'n_classes': PICTURE_SIZE,\n",
    "          'n_channels': 1,\n",
    "          'shuffle': True}\n",
    "\n",
    "with open(PARTITION_PATH, \"rb\") as file:\n",
    "    partition = pickle.load(file)\n",
    "\n",
    "with open(LABELS_PATH, \"rb\") as file:\n",
    "    labels = pickle.load(file)\n",
    "\n",
    "# Generators\n",
    "training_generator = DataGenerator(partition['train'], labels, **params, data_gen = True)\n",
    "validation_generator = DataGenerator(partition['validation'], labels, **params, data_gen = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "glTj3sqWTC_e"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(PATCH_SIZE, PATCH_SIZE, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Flatten(),\n",
    "\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(PICTURE_SIZE)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "owuTfJu-I8Sk",
    "outputId": "9d50dacb-5b97-4235-f58b-3ca25d103e84"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FluaDrKNqP-b",
    "outputId": "63d85bbe-5191-4729-ecac-89cb73c55583"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "filepath = \"/content/model_at_epoch_{epoch}.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto', save_freq='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rPtz-PbBe8zZ"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V5Nac1yU5gbo",
    "outputId": "474d4bb0-3931-4de7-ced1-c61234affb35"
   },
   "outputs": [],
   "source": [
    "# This Version Is For Not Saving The Model Throughout The Training Process\n",
    "\n",
    "# history = model.fit(training_generator,\n",
    "#           validation_data=validation_generator,\n",
    "#           epochs=EPOCHS,\n",
    "#           workers=1)\n",
    "\n",
    "history = model.fit(training_generator,\n",
    "          validation_data=validation_generator,\n",
    "          epochs=EPOCHS,\n",
    "          workers=1,\n",
    "          callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XpN5wwPu5A3i",
    "outputId": "f08c6b23-00ae-4319-bfe4-0386f158922b"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tests_predictions_lists_by_img = []\n",
    "tests_predictions_by_img = []\n",
    "tests_lables_by_img = []\n",
    "for idx, sublist in enumerate(partition['test']):\n",
    "  tests_patches = []\n",
    "  tests_lables_by_img.append(idx)\n",
    "  for ID in sublist:\n",
    "    filepath = FILE_PATH\n",
    "    tests_patches.append(np.expand_dims(np.load(filepath + ID + '.npy'), axis=-1))\n",
    "\n",
    "  predictions = model.predict(np.array(tests_patches))\n",
    "  predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "  tests_predictions_lists_by_img.append(predictions)\n",
    "\n",
    "  tests_predictions_by_img.append(predicted_classes) # save only the prediction of the patches to save in memory\n",
    "\n",
    "all_tests_predictions = []\n",
    "all_tests_labels = []\n",
    "for idx, sublist in enumerate(tests_predictions_by_img):\n",
    "  all_tests_predictions.extend(sublist)\n",
    "  all_tests_labels.extend([idx] * len(sublist))\n",
    "\n",
    "test_acc = accuracy_score(all_tests_labels, all_tests_predictions)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5fgKkF3kOEd2"
   },
   "outputs": [],
   "source": [
    "predictions_by_images = np.array([])\n",
    "for predictions in tests_predictions_by_img:\n",
    "  most_common = np.bincount(predictions).argmax()\n",
    "  predictions_by_images = np.append(predictions_by_images, most_common)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "4rjASQKv_U53",
    "outputId": "53ef1521-ceff-405a-f77e-c5198e2c048a"
   },
   "outputs": [],
   "source": [
    "# Here we sum all the predictions into one set of predictions and then find the maximum.\n",
    "# It is more successful than the previous method.\n",
    "\n",
    "predictions_by_sum_method = np.array([])\n",
    "\n",
    "for predictions in tests_predictions_lists_by_img:\n",
    "  sum_predictions = np.sum(predictions, axis=0)\n",
    "  predicted_classes_sum = np.argmax(sum_predictions, axis=0)\n",
    "  predictions_by_sum_method = np.append(predictions_by_sum_method, predicted_classes_sum)\n",
    "\n",
    "misclassified_imgs1 = []\n",
    "\n",
    "for i in range(len(predictions_by_sum_method)):\n",
    "  if i != int(predictions_by_sum_method[i]):\n",
    "    misclassified_imgs1.append((i, int(predictions_by_sum_method[i])))\n",
    "\n",
    "# Save to a pkl file\n",
    "with open('misclassified_imgs.pkl', 'wb') as f:\n",
    "    pickle.dump(misclassified_imgs1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "fszjlzNO1aZm",
    "outputId": "6467eb90-038b-4d86-8341-3c92d711e856"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "zYgu0Jo_kDD4",
    "outputId": "a22bd684-e181-44f5-892a-24fb11c724b0"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create the confusion matrix\n",
    "most_common_cm = confusion_matrix(tests_lables_by_img, predictions_by_images)\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(40, 40))\n",
    "sns.heatmap(most_common_cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.title(\"Confusion Matrix of the Most Common Prediction Method Accuracy Score\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Create the confusion matrix\n",
    "sum_predictions_cm = confusion_matrix(tests_lables_by_img, predictions_by_sum_method)\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(40, 40))\n",
    "sns.heatmap(sum_predictions_cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.title(\"Confusion Matrix of the Sum Predictions Method Accuracy Score\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "num_of_patches_cm = confusion_matrix(all_tests_labels, all_tests_predictions)\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(60, 60))\n",
    "sns.heatmap(num_of_patches_cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.title(\"Confusion Matrix of Amount of incorrect and correct patches classified\", fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "39TJ-AV-YDEt",
    "outputId": "526dbcf8-5b75-4444-df56-ecf4ba9cf0d5"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('Picture Size: ', PICTURE_SIZE)\n",
    "print('Epochs: ', EPOCHS)\n",
    "print('Learning Rate: ', LEARNING_RATE)\n",
    "print('Batch Size: ', BATCH_SIZE, '\\n')\n",
    "print('Results: ')\n",
    "print('Test:  ', round(test_acc, 3))\n",
    "print('Accuracy Score', round(accuracy_score(tests_lables_by_img, predictions_by_images), 3))\n",
    "print('Sum Predictions Accuracy Score', round(accuracy_score(tests_lables_by_img, predictions_by_sum_method), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_qeIC8s9h0X6",
    "outputId": "08d90980-d4f2-48e0-a63f-02726169f541",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "\n",
    "def deprocess_image(x):\n",
    "    x -= tf.reduce_mean(x)\n",
    "    x /= (tf.math.reduce_std(x) + 1e-5)\n",
    "    x *= 0.1\n",
    "    x += 0.5\n",
    "    x = tf.clip_by_value(x, 0, 1)\n",
    "    x *= 255\n",
    "    x = tf.clip_by_value(x, 0, 255)\n",
    "    return tf.cast(x, tf.uint8)\n",
    "\n",
    "def generate_pattern(layer_name, filter_index, model_new, size=PATCH_SIZE):\n",
    "    def calculate_loss_and_grads(input_image):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(input_image)\n",
    "            loss = tf.reduce_mean(model_new(input_image)[:, :, :, filter_index])\n",
    "        grads = tape.gradient(loss, input_image)\n",
    "        grads /= (tf.sqrt(tf.reduce_mean(tf.square(grads))) + 1e-5)\n",
    "        return loss, grads\n",
    "\n",
    "    input_img_data = tf.random.uniform((1, size, size, 1)) * 20 + 128\n",
    "    input_img_data = tf.Variable(input_img_data)\n",
    "    step = 1.\n",
    "    for i in range(100):\n",
    "        loss_value, grads_value = calculate_loss_and_grads(input_img_data)\n",
    "        input_img_data.assign_add(grads_value * step)\n",
    "    img = input_img_data[0]\n",
    "    return deprocess_image(img)\n",
    "\n",
    "def plot_max_neuron_reaction(layer_name, neuron_num, fig_num=32):\n",
    "  filter_index = 0\n",
    "  layer_output = model.get_layer(layer_name).output\n",
    "  loss = K.mean(layer_output[:, :, :, filter_index])\n",
    "\n",
    "  input_img_data = tf.random.uniform((1, PATCH_SIZE, PATCH_SIZE, 1)) * 20 + 128\n",
    "  input_img_data = tf.Variable(input_img_data)\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "      tape.watch(input_img_data)\n",
    "      loss = model(input_img_data)\n",
    "\n",
    "  grads = tape.gradient(loss, input_img_data)\n",
    "  grads /= (tf.sqrt(tf.reduce_mean(tf.square(grads))) + 1e-5)\n",
    "\n",
    "  step = 1.\n",
    "  for i in range(40):\n",
    "      with tf.GradientTape() as tape:\n",
    "          tape.watch(input_img_data)\n",
    "          loss = model(input_img_data)\n",
    "\n",
    "      grads = tape.gradient(loss, input_img_data)\n",
    "      grads /= (tf.sqrt(tf.reduce_mean(tf.square(grads))) + 1e-5)\n",
    "      input_img_data.assign_add(grads * step)\n",
    "\n",
    "  model_new = Model(inputs=model.inputs, outputs=model.get_layer(layer_name).output)\n",
    "\n",
    "  plt.figure(figsize=(20, 20) )\n",
    "  plt.suptitle(layer_name, fontsize=16)\n",
    "  for i in range(fig_num):\n",
    "      plt.subplot(8, 8, i+1)\n",
    "      plt.xticks([])\n",
    "      plt.yticks([])\n",
    "      plt.grid(False)\n",
    "      plt.imshow(generate_pattern(layer_name, i,model_new ))\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "for layer in model.layers:\n",
    "  if layer.name.startswith(\"conv2d\") or layer.name.startswith(\"max_pooling2d\"):\n",
    "    plot_max_neuron_reaction(layer.name, neuron_num=32)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
